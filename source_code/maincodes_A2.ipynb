{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment tracking\n",
    "import mlflow\n",
    "import os\n",
    "# This the dockerized method.\n",
    "# We build two docker containers, one for python/jupyter and another for mlflow.\n",
    "# The url `mlflow` is resolved into another container within the same composer.\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "# In the dockerized way, the user who runs this code will be `root`.\n",
    "# The MLflow will also log the run user_id as `root`.\n",
    "# To change that, we need to set this environ[\"LOGNAME\"] to your name.\n",
    "os.environ[\"LOGNAME\"] = \"reza370\"\n",
    "mlflow.create_experiment(name=\"momentum 0.9 and stochastic\")  #create if you haven't create\n",
    "mlflow.set_experiment(experiment_name=\"momentum 0.9 and stochastic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_cars=pd.read_csv('Cars.csv')\n",
    "owner_coding = {\n",
    "    'First Owner': 1,\n",
    "    'Second Owner': 2,\n",
    "    'Third Owner': 3,\n",
    "    'Fourth & Above Owner': 4,\n",
    "    'Test Drive Car': 5\n",
    "}\n",
    "df_cars['owner'] = df_cars['owner'].map(owner_coding)\n",
    "#2\n",
    "df_cars = df_cars[df_cars['fuel'].isin(['Petrol', 'Diesel'])]\n",
    "#3\n",
    "df_cars.mileage = df_cars.mileage.str.split(expand=True)[0].astype(float)\n",
    "#4\n",
    "df_cars.engine = df_cars.engine.str.split(expand=True)[0].astype(float)\n",
    "#5\n",
    "df_cars.loc[df_cars['max_power'] == 'bph', 'max_power'] = ' bph'\n",
    "df_cars.max_power = df_cars.max_power.str.split(expand=True)[0].astype(float)\n",
    "#6\n",
    "df_cars.name=df_cars.name.str.split(expand=True)[0]\n",
    "#7\n",
    "df_cars = df_cars.drop(columns=['torque'])\n",
    "#8\n",
    "df_cars = df_cars[df_cars['owner'] != 5]\n",
    "#9\n",
    "import numpy as np\n",
    "df_cars['selling_price'] = np.log(df_cars['selling_price'])\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "df_cars['car_age'] = int(now.strftime(\"%Y\")) - df_cars['year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cars[['max_power', 'mileage', 'car_age']]\n",
    "y = df_cars['selling_price']\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 370)\n",
    "X_train['max_power'].fillna(X_train['max_power'].median(), inplace=True)\n",
    "X_train['mileage'].fillna(X_train['mileage'].mean(), inplace=True)\n",
    "X_test['max_power'].fillna(X_train['max_power'].median(), inplace=True)\n",
    "X_test['mileage'].fillna(X_train['mileage'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['max_power', 'mileage', 'car_age']\n",
    "label_name = 'selling_price'\n",
    "from   sklearn.preprocessing import StandardScaler\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "from   time import time\n",
    "import pandas as pd\n",
    "scaler  = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) \n",
    "X_test  = scaler.transform(X_test)\n",
    "X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "X_test  = np.insert(X_test, 0, 1, axis=1)\n",
    "y_train = np.array(y_train)\n",
    "y_test  = np.array(y_test)\n",
    "poly_X_train = PolynomialFeatures(degree = 2, include_bias=False).fit_transform(X_train)[:, [4,5,6,7,8,9,10,11,12,13]]\n",
    "poly_X_test = PolynomialFeatures(degree = 2, include_bias=False).fit_transform(X_test)[:, [4,5,6,7,8,9,10,11,12,13]]\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape,  poly_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(poly_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class LinearRegression(object):\n",
    "    \n",
    "    kfold = KFold(n_splits=3)\n",
    "            \n",
    "    def __init__(self, regularization, lr=0.001, method='batch', num_epochs=100, batch_size=50, cv=kfold, weight_init = 'zeros', momentum = 0.9):\n",
    "        self.lr             = lr\n",
    "        self.num_epochs     = num_epochs\n",
    "        self.batch_size     = batch_size\n",
    "        self.method         = method\n",
    "        self.cv             = cv\n",
    "        self.weight_init    = weight_init\n",
    "        self.momentum       = momentum\n",
    "        self.regularization = regularization\n",
    "        self.prev_grad      = None  \n",
    "\n",
    "    def mse(self, ytrue, ypred):\n",
    "        return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "    \n",
    "    #:::::::::::::::Assignment 2, Task1.1:Add a function r2 that compute the r2score.:::::::::::::\n",
    "    def r_squared(self, y_true, y_pred):\n",
    "        ss_residual = ((y_pred - y_true) ** 2).sum()\n",
    "        ss_total = (((y_true) - np.mean(y_true)) ** 2).sum()\n",
    "        r2 = 1 - (ss_residual / ss_total)\n",
    "        return r2\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "            \n",
    "        #create a list of kfold scores\n",
    "        self.kfold_scores = list()\n",
    "        \n",
    "        #reset val loss\n",
    "        self.val_loss_old = np.infty\n",
    "\n",
    "        #kfold.split in the sklearn.....\n",
    "        #5 splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            \n",
    "            X_cross_train = X_train[train_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            X_cross_val   = X_train[val_idx]\n",
    "            y_cross_val   = y_train[val_idx]\n",
    "                   \n",
    "    #::::::::::::::Assignment 2, Task1.2: Modify the class such that it allows the user to choose between zeros initialization or xavier.::::::::::::::\n",
    "\n",
    "            # \"Xavier/Glorot\" or \"zero\" initialization\n",
    "            if self.weight_init == 'xavier':\n",
    "                limit = np.sqrt(1 / (X_cross_train.shape[0]))\n",
    "                self.theta = np.random.uniform(-limit, limit, size=X_cross_train.shape[1])\n",
    "            if self.weight_init == 'zeros':\n",
    "                self.theta = np.zeros(X_cross_train.shape[1])\n",
    "                              \n",
    "            #define X_cross_train as only a subset of the data\n",
    "            #how big is this subset?  => mini-batch size ==> 50\n",
    "\n",
    "                     \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            with mlflow.start_run(run_name=f\"Fold-{fold}\", nested=True):\n",
    "                \n",
    "                params = {\"method\": self.method, \"lr\": self.lr, \"reg\": type(self).__name__, \"initial weight\":self.weight_init}\n",
    "                mlflow.log_params(params=params)\n",
    "                \n",
    "                for epoch in range(self.num_epochs):\n",
    "                \n",
    "                    #with replacement or no replacement\n",
    "                    #with replacement means just randomize\n",
    "                    #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "                    #shuffle your index\n",
    "                    perm = np.random.permutation(X_cross_train.shape[0])\n",
    "                            \n",
    "                    X_cross_train = X_cross_train[perm]\n",
    "                    y_cross_train = y_cross_train[perm]\n",
    "                    \n",
    "                    if self.method == 'sto':\n",
    "                        for batch_idx in range(X_cross_train.shape[0]):\n",
    "                            X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
    "                            y_method_train = y_cross_train[batch_idx] \n",
    "                            train_loss = self._train(X_method_train, y_method_train)\n",
    "                    elif self.method == 'mini':\n",
    "                        for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
    "                            #batch_idx = 0, 50, 100, 150\n",
    "                            X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
    "                            y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                            train_loss = self._train(X_method_train, y_method_train)\n",
    "                    else:\n",
    "                        X_method_train = X_cross_train\n",
    "                        y_method_train = y_cross_train\n",
    "                        train_loss = self._train(X_method_train, y_method_train)\n",
    "\n",
    "                    mlflow.log_metric(key=\"train_loss\", value=train_loss, step=epoch)\n",
    "\n",
    "                    yhat_val = self.predict(X_cross_val)\n",
    "                    val_loss_new = self.mse(y_cross_val, yhat_val)\n",
    "                    #val_loss_new = self.r_squared(y_cross_val, yhat_val)\n",
    "                    mlflow.log_metric(key=\"val_loss\", value=val_loss_new, step=epoch)\n",
    "                    \n",
    "                    #record dataset\n",
    "                    #mlflow_train_data = mlflow.data.from_numpy(features=X_method_train, targets=y_method_train)\n",
    "                    #mlflow.log_input(mlflow_train_data, context=\"training\")\n",
    "                    \n",
    "                    #mlflow_val_data = mlflow.data.from_numpy(features=X_cross_val, targets=y_cross_val)\n",
    "                    #mlflow.log_input(mlflow_val_data, context=\"validation\")\n",
    "                    \n",
    "                    #early stopping\n",
    "                    if np.allclose(val_loss_new, self.val_loss_old):\n",
    "                        break\n",
    "                    self.val_loss_old = val_loss_new\n",
    "            \n",
    "                self.kfold_scores.append(val_loss_new)\n",
    "                print(f\"Fold {fold}: {val_loss_new}\")\n",
    "    \n",
    "        \n",
    "                    \n",
    "    def _train(self, X, y):\n",
    "                    \n",
    "        yhat = self.predict(X)\n",
    "        m    = X.shape[0]        \n",
    "        grad = (1/m) * X.T @(yhat - y) + self.regularization.derivation(self.theta)\n",
    "        \n",
    "        #::::::::::::::Assignment 2, Task1.3: Modify the class such that it allows the user to choose momentum.::::::::::::::\n",
    "      \n",
    "        if self.prev_grad is None:\n",
    "            self.prev_grad = np.zeros_like(self.theta)  # Initialize velocity if it's None\n",
    "\n",
    "        # Update the velocity using momentum\n",
    "        self.prev_grad = self.momentum * self.prev_grad + (1 - self.momentum) * grad\n",
    "\n",
    "        # Update the parameters using the prev_grad and learning rate\n",
    "        self.theta = self.theta - self.lr * self.prev_grad\n",
    "        return self.mse(y, yhat) \n",
    "           \n",
    "        \n",
    "        #self.theta = self.theta - self.lr * grad\n",
    "        #return self.mse(y, yhat)\n",
    "        #return self.r_squared(y, yhat)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    def plot_feature_importance(self, feature_names):\n",
    "        # Create a list of tuples with feature names and coefficients\n",
    "        feature_coef = [(name, coef) for name, coef in zip(feature_names, self.theta)]\n",
    "\n",
    "        # Sort the list by coefficient magnitude\n",
    "        feature_coef.sort(key=lambda x: abs(X_train[1]), reverse=True)\n",
    "\n",
    "        # Extract sorted feature names and coefficients\n",
    "        sorted_feature_names, sorted_coefficients = zip(*feature_coef)\n",
    "\n",
    "        # Create a horizontal bar plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(sorted_feature_names, sorted_coefficients, color='b')\n",
    "        plt.xlabel('Coefficient Magnitude')\n",
    "        plt.title('Feature Importance Based on Coefficients')\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis for better readability\n",
    "        plt.show()\n",
    "\n",
    "   \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta  #===>(m, n) @ (n, )\n",
    "    \n",
    "    def _coef(self):\n",
    "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
    "                               #w0 is the bias or the intercept\n",
    "                               #w1....wn are the weights / coefficients / theta\n",
    "    def _bias(self):\n",
    "        return self.theta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalpenalty:\n",
    "    def __init__(self, l):\n",
    "        self.l = 0 # lambda value\n",
    "        \n",
    "    def __call__(self, theta): #__call__ allows us to call class as method\n",
    "        return self.l * np.sum(np.abs(theta))\n",
    "        \n",
    "    def derivation(self, theta):\n",
    "        return self.l * np.sign(theta)\n",
    "\n",
    "class LassoPenalty:\n",
    "    \n",
    "    def __init__(self, l):\n",
    "        self.l = l # lambda value\n",
    "        \n",
    "    def __call__(self, theta): #__call__ allows us to call class as method\n",
    "        return self.l * np.sum(np.abs(theta))\n",
    "        \n",
    "    def derivation(self, theta):\n",
    "        return self.l * np.sign(theta)\n",
    "    \n",
    "class RidgePenalty:\n",
    "    \n",
    "    def __init__(self, l):\n",
    "        self.l = l\n",
    "        \n",
    "    def __call__(self, theta): #__call__ allows us to call class as method\n",
    "        return self.l * np.sum(np.square(theta))\n",
    "        \n",
    "    def derivation(self, theta):\n",
    "        return self.l * 2 * theta\n",
    "    \n",
    "class ElasticPenalty:\n",
    "    \n",
    "    def __init__(self, l = 0.1, l_ratio = 0.5):\n",
    "        self.l = l \n",
    "        self.l_ratio = l_ratio\n",
    "\n",
    "    def __call__(self, theta):  #__call__ allows us to call class as method\n",
    "        l1_contribution = self.l_ratio * self.l * np.sum(np.abs(theta))\n",
    "        l2_contribution = (1 - self.l_ratio) * self.l * 0.5 * np.sum(np.square(theta))\n",
    "        return (l1_contribution + l2_contribution)\n",
    "\n",
    "    def derivation(self, theta):\n",
    "        l1_derivation = self.l * self.l_ratio * np.sign(theta)\n",
    "        l2_derivation = self.l * (1 - self.l_ratio) * theta\n",
    "        return (l1_derivation + l2_derivation)\n",
    "\n",
    "class Normal(LinearRegression):\n",
    "    def __init__(self, method, lr, l):\n",
    "        self.regularization = Normalpenalty(l)\n",
    "        super().__init__(self.regularization, lr, method)\n",
    "\n",
    "class Lasso(LinearRegression):\n",
    "    \n",
    "    def __init__(self, method, lr, l):\n",
    "        self.regularization = LassoPenalty(l)\n",
    "        super().__init__(self.regularization, lr, method)\n",
    "        \n",
    "class Ridge(LinearRegression):\n",
    "    \n",
    "    def __init__(self, method, lr, l):\n",
    "        self.regularization = RidgePenalty(l)\n",
    "        super().__init__(self.regularization, lr, method)\n",
    "        \n",
    "class ElasticNet(LinearRegression):\n",
    "    \n",
    "    def __init__(self, method, lr, l, l_ratio=0.5):\n",
    "        self.regularization = ElasticPenalty(l, l_ratio)\n",
    "        super().__init__(self.regularization, lr, method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for looping classnames\n",
    "import sys\n",
    "\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = [\"Normal\"]\n",
    "\n",
    "for reg in regs:\n",
    "\n",
    "    params = {\"method\": \"mini\", \"lr\": 0.1, \"l\": 0,}\n",
    "    mlflow.start_run(run_name=f\"method-{params['method']}-lr-{params['lr']}-reg-{'polynomial'}-weight_init-{'zeros'}-momentum{'Yes'}\", nested=True)\n",
    "    print(\"*\"*5, \"Polynomial\", \"*\"*5)\n",
    "     # #######\n",
    "    type_of_regression = str_to_class(reg)    \n",
    "    model = type_of_regression(**params)\n",
    "    model.fit(poly_X_train, y_train)\n",
    "    yhat = model.predict(poly_X_test)\n",
    "    mse  = model.mse(yhat, y_test)\n",
    "    r_squared = model.r_squared(y_test, yhat)\n",
    "    print(\"Test mse: \", mse)\n",
    "    mlflow.log_metric(key=\"mse\", value=mse)\n",
    "    print(\"Test R2: \", r_squared)\n",
    "    \n",
    "    \n",
    "    mlflow.log_metric(key=\"r_squared\", value=r_squared)\n",
    "    signature = mlflow.models.infer_signature(poly_X_train, model.predict(poly_X_train))\n",
    "    mlflow.sklearn.log_model(model, artifact_path='model', signature=signature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = [\"Normal\", \"Ridge\", \"Lasso\"]\n",
    "\n",
    "for reg in regs:\n",
    "\n",
    "    params = {\"method\": \"sto\", \"lr\": 0.1, \"l\": 0.1,}\n",
    "    mlflow.start_run(run_name=f\"method-{params['method']}-lr-{params['lr']}-reg-{reg}-weight_init-{'zeros'}-momentum{'Yes'}\", nested=True)\n",
    "    print(\"*\"*5, reg, \"*\"*5)\n",
    "     # #######\n",
    "    type_of_regression = str_to_class(reg)    #Normal, Ridge, Lasso\n",
    "    model = type_of_regression(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    mse  = model.mse(yhat, y_test)\n",
    "    r_squared = model.r_squared(y_test, yhat)\n",
    "    print(\"Test mse: \", mse)\n",
    "    mlflow.log_metric(key=\"mse\", value=mse)\n",
    "    print(\"Test R2: \", r_squared)\n",
    "\n",
    "    \n",
    "    mlflow.log_metric(key=\"r_squared\", value=r_squared)\n",
    "    signature = mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
    "    mlflow.sklearn.log_model(model, artifact_path='model', signature=signature)\n",
    "\n",
    "    # #######\n",
    "\n",
    "    mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
